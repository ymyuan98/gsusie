---
title: "Quick Demo 2: Modelling count-type responses via Poisson regression based GSuSiE model"
author: "Ming Yuan"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Quick Demo 2: Modelling count-type responses via Poisson regression based GSuSiE model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
editor_options: 
  markdown: 
    wrap: 72
bibliography: references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, comment = "#>",
  fig.width = 5, fig.height = 4, fig.align = "center", fig.cap = "&nbsp;", 
  dpi = 120
)
```

In this vignette, we show how to apply the `gsusie()` function on a Poisson
regression based GSuSiE model dedicated to handling count-type responses.

## Get genotype data and generate count-type responses

We simulate response using genotype data from an R package named
[bigsnpr](https://github.com/privefl/bigsnpr/).
The explanatory variables `X` in this example, 
saved in `./data/genotype/genotype-subset-2.rds`, 
is a subset of genotype data containing 2000 successive SNPs of 500 individuals. 
The data are extracted using the following code: 

```{r, eval = F}
# install.packages("bigsnpr")
library(bigsnpr)

`%&%` <- function(a,b) paste0(a,b)
data.dir <- "../data/genotype"
dir.create(data.dir, recursive=TRUE, showWarnings=FALSE)

.bed.file <- data.dir %&% "1000G_phase3_common_norel.bed"
if (!file.exists(.bed.file)) {
  bigsnpr::download_1000G(data.dir)
}

.bk.file <- data.dir %&% "1000G_phase3_common_norel.rds"
if (!file.exists(.bk.file)){
  BED <- snp_readBed(.bed.file)
}
dat <- snp_attach(.bk.file)$genotypes

nn <- 500
pp <- 2000

set.seed(123456789)
startpoint <- sample(1 : (ncol(dat)-pp), size = 1)
if (nn < 2490) {ii.idx <- sample(1 : 2490, size = nn)}
X <- dat[ii.idx, startpoint : (startpoint+pp-1)]

example.data.dir <- "./example-data/"
saveRDS(X, file = example.data.dir %&% "genotype-subset-2.rds")
```

Three effect variables are randomly selected, each having a non-zero
effect size of 1 (before scaling). 
Then, we generate the linear predictor `eta`. 
We scale the linear combination to avoid unexpected large or small values in `eta`.

```{r}
`%&%` <- function(a,b) paste0(a,b)
example.data.dir <- "./example-data/"

X <- readRDS(example.data.dir %&% "genotype-subset-2.rds")
nn <- nrow(X)  # 500
pp <- ncol(X)  # 2000

n_effect_vars <- 3

set.seed(12345)

## independent variables with non-zero effects
effect_idx <- sample(1:pp, size = n_effect_vars)
print(effect_idx)

## effect size
effect_size <- rep(1, times = n_effect_vars)

## linear predictor
eta <- scale(X[,effect_idx, drop=F] %*% as.matrix(effect_size))

## response
y <- rpois(nn, exp(eta))
```

The histogram of the synthetic response is:

```{r}
hist(y, breaks = 12)
```

## Fit a Poisson regression based GSuSiE model

When fitting a GLM-based GSuSiE model, we prefer adding an all-one column to 
introduce an intercept term. In practice, we are inclined to add this auxiliary 
column to the end of the design matrix X such that variable indices would not 
be confusing when printing the summarized results by the `summary()` function. 
This step is optional; nevertheless, we find that, by adding this all-one 
column, the model is less likely to be overfitted. (Wooh!)

By default, we assume there are no more than 10 effect variables
and set `maxL = min(10, ncol(X))`. You may choose to specify another
value to `maxL`.

To specify fitting a **Poisson regression based GSuSiE model**, 
use the argument `family = "poisson"`. Correspondingly, the
`gsusie()` function call is:

```{r}
library(gsusie)
res_gs <- gsusie(cbind(X, 1), y, family = "poisson")
```

## Check the variable selection results

### Posterior inclusion probabilities

The `print_gsusie_coefficients()` function offers 10 (by default)
variables with the highest posterior inclusion probabilities (PIPs). By
default, we output 95% credible intervals for the posterior means of each
variable.

```{r}
print_gsusie_coefficients(res_gs)
```

True effect variables (`X51`, `X142`, and `X720`) are detected with high PIPs. 
However, null variables `X1321` and `X134` are also detected, and their 
corresponding PIPs are also close to 1. 
Well, the result is not that satisfying, as two out of five detected 
variables are not effect variables. 

### Credible sets (CS)

As with the SuSiE method, we output 95% credible sets by default.

```{r}
print(res_gs$sets)
```

Each credible set contain one single variable, and two of them contains 
null variables `X1321` and `X134`. 

### Graphical Display

The `gsusie_plot()` function provides plots for PIP of each variable.

By default, since the intercept is just an auxiliary variable, we do not
include it in the plot. In this case, argument `intercept_index` needs
to be specified. (P.S. By default, we set `include_intercept = FALSE` to
remove the intercept term on the plot.)

```{r}
gsusie_plot(res_gs, y = "PIP", 
            include_intercept = FALSE, 
            intercept_index = (pp+1), 
            effect_indices = effect_idx)
```

By specifying `effect_indices`, the true effect variables are
colored in red. The 95% credible sets are identified as circled in
different colors.

The plot also vividly presents that the null variables `X1321` and `X134 are
mistakenly selected because of their high PIPs. 
Therefore, we hope to address these false discoveries.

## Refine a Poisson regression based GSuSiE model via robust estimation

As we know, count data (in the response) usually contains extremely
large values. These unexpected values may affect the model fitting,
resulting in false discoveries or even failure to fit the model.

Hence, we suggest robust estimation when modelling a Poisson
regression based GSuSiE model. Our `gsusie` function provides several
robust estimation approaches. Based on our experience in simulation
experiments, we recommend the **Huber reweighting method**, which, in each
iteration, reweights each data point by Huber weight[@huber1964]
according to its residual updated from the previous iteration [(by
M-estimation)]{style="color: gray;"}.

The corresponding function call is:

```{r}
res_gs_hb <- gsusie(cbind(X, 1), y, family = "poisson", 
                    robust_estimation = TRUE,
                    robust_method = "huber")
```

### Check the refined variable selection results

The fitted results as well as graphical display are:

```{r}
print_gsusie_coefficients(res_gs_hb)
```

```{r}
summary(res_gs_hb)
```

```{r}
print(res_gs_hb$sets)
```


```{r}
gsusie_plot(res_gs_hb, y = "PIP", 
            include_intercept = FALSE, 
            intercept_index = (pp+1), 
            effect_indices = effect_idx)
```

From above, we notice that, by applying the robust Huber reweighting
method, the selection accuracy is somehow improved: the PIPs of `X1321` and
`X134` no longer deviate from 0 and thus are not able to
be falsely discovered. 
Instead, the null variable `X54` is detected along with the true effect variable 
`X51`, but the PIP of the former is relatively lower than that of the latter. 
Besides, as suggested above, 
`X51` is captured by two different credible sets, 
one of which contains only `X51`. 
This may indicate that `X51` is likely to be more important than `X54`. 

Well, to me, 
the refined results are at least more gratifying than the unrefined ones. 

Other than performing Huber reweighting via M-estimation procedures, we
also provide an option of Huber reweighting via
S-estimation[@rousseeuw1984]. The corresponding function call is:

```{r}
res_gs_hb2 <- gsusie(cbind(X, 1), y, family = "poisson", 
                     robust_estimation = TRUE, 
                     robust_method = "huber", 
                     robust_tuning_method = "S")
```

The fitted results as well as graphical display are:

```{r}
print_gsusie_coefficients(res_gs_hb2)
```

```{r}
summary(res_gs_hb2)
```


```{r}
gsusie_plot(res_gs_hb2, y = "PIP", 
            include_intercept = FALSE, 
            intercept_index = (pp+1), 
            effect_indices = effect_idx)
```

In simulation experiments, results yielded from Huber reweighting
via both M- and S-estimation procedures are alike.

> Our previous experience suggests that the Bisquare reweighting method
> is usually too conservative to offer as much informative information
> as the Huber reweighting method. We do not recommend simply 
> removing a proportion of data points whose pseudo-weights or
> pseudo-residuals exceed a certain threshold, as it is difficult to
> tell whether this specific threshold is appropriate or not.

## Suggestions for preprocessing count-type responses

Two cases in the count data response that may result in
difficulty fitting a Poisson regression based GSuSiE model. One is
the presence of too many zeros, and the other is the existence of
some unexpectedly large values. The former can be addressed by fitting a
zero-inflated Poisson or a negative binomial regression model;
however, the GSuSiE versions of these two genearlized linear models have not 
yet been developed in our `gsusie` package. 
The latter may lead to unexpected failure of fitting a GSuSiE model, 
as those extreme values may lead to overflow in certain fitting procedure. 

A simple approach to simultaneously address both problems is to preprocess the response: 
we scale the response without changing the shape of its original distribution. 
Specifically, the following transformation is operated  

```{r, eval = F}
y <- exp(log1p(y))
```

and then the new $y$ is plugged into the `gsusie` function to fit a GSuSiE model. 
This transformation on $y$ is particularly beneficial in reducing the chance of 
failure in model fitting. 

## References 



